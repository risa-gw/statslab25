---
title: "SOC-GA 2332 Intro to Stats Lab 5"
author: "Risa Gelles-Watnick"
date: "10/03/2025"
output:
  pdf_document
urlcolor: blue
---

# Logistics & Announcement

* Everyone got an 100% on Problem Set #1. Great job!

* **Problem Set #2** will be distributed today and is due October 17th. This one will take longer than the last so I would recommend starting earlier.

* Since office hours conflicts with the Goddard Seminar, please email me if you want to meet at office hours this week.

# Review of Problem Set and Quiz

## Common issues from Problem Set #1

### 1. Standard Error vs. Standard Deviation

* **Standard deviation:** $s = \sqrt{\frac{1}{n-1}\sum_{i=1}^n(y_i-\bar{y})^2}$

* **Standard error:** $\text{SE} = \frac{\sigma}{\sqrt{n}}$

    * In practice, we use the sample standard deviation $s$ as a stand-in for the population standard deviation $\sigma$. 

* *Exercise:* Explain in your own words what each of these formulas are calculating conceptually. (5 minutes)

### 2. P-values

* P-values are the probability, given that the null hypothesis is true, that you would observe a test statistic equal to the one you observe *or more extreme*. 

# Review of OLS Regressions

- Regression is one of the most common tool social scientists use to understand the relationships between variables. 

- *Question:* What is a regression conceptually?

- This week we started on the simplest regression that consists of one outcome ($Y_i$) and one predictor ($X_i$) with the parameters $\beta_0$, $\beta_1$, and the error term $\epsilon_i$. This is called the **bivariate regression**. Consider the underlying **Data Generating Process** (DGP) in the **population**, where

$$Y_i = \beta_0 + \beta_1X_i + \epsilon_i$$
- We never observe the population. Instead, we have a **sample** and statistical methods to estimate the value of $\beta_0$ and $\beta_1$ given observed values of $y_i$ and $x_i$ in the **sample**. This method of estimation is called the **Ordinary Least Squares** (OLS) estimation.

- The OLS estimators of $\beta_0$ and $\beta_1$ given the sample *minimize* the sum of squared errors (SSE).

$$
SSE = \sum^{n}_{i=1}e_i^2
$$
$$
SSE = \sum^{n}_{i=1} \left(y_i - (\hat\beta_0 + \hat\beta_1 x_i) \right) ^2
$$
## Running OLS regressions in R

```{r setup, include=T,message=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# loading packages for today's lab
pacman::p_load(
  tidyverse,
  foreach,
  stargazer,
  ggcorrplot,
  psych
)
```

Estimating linear models using OLS is very simple in `R`. We can use the linear model function `lm()` to estimate regression models. However, you should always remember that OLS estimation entails a set of assumptions about your data that you should always check before estimating your models. 

## Gauss-Markov Assumptions: 

1. Linear in parameters
2. Random sampling
3. No perfect collinearity
4. Zero conditional mean (Do we force this to be true in estimating OLS? No! We hope this is true in the DGP such that our OLS estimation is consistent and unbiased)
5. Homoskedasticity (error term has a constant variance across the value of $x$): $\mathrm{Var}(\epsilon\mid x_1, x_2, \dots, x_k) = \sigma^2$

## Estimate OLS in R

Let's practice running an OLS regression in R. First, we'll create a simulated dataset. 

```{r lm}
# set seed
set.seed(2626)

# creating a toy dataset
x = runif(n = 500) * 5
y = 3 + 5 * x + rnorm(n = 500, mean = 0, sd = 1)
df <- data.frame(x, y)
```

A good first step is always plotting the relationship between your two variables of interest. Let's see if they seem to be linearly related and what the direction of the relationship is. This is also a good time to look for outliers. 

```{r}
df %>% 
ggplot(., aes(x = x, y = y)) +
  geom_point(color = "grey20", alpha = 0.6) + 
  labs(title = "X & Y Linearly Related") +
  theme_minimal()
```
We can see that X & Y seem to be linearly related (and seem to be directly related). This is a good sign that we should try a linear regression! Even if linear regressions aren't a perfect fit, they often do a pretty good job and are very easy to interpret and explain to others. 

To run a linear regression in R:

```{r}
## estimate a linear model
ols1 <- df %>% 
  lm(y ~ x, data = .)

## check model result
ols1

## for details, use the summary() function
summary(ols1)

## the SSE is not directly displayed, but we can calculate it by its definition
sum(ols1$residuals^2)
```

How do we interpret these results? 

We can visualize this regression by adding a regression line to our scatter plot.

```{r}
df %>%
  ggplot(., aes(x = x, y = y)) +
  geom_point(color = "grey20", alpha = 0.6) +
  geom_smooth(
    method = "lm",
    se = FALSE,
    color = "blue",
    size = 1.2
  ) +
  labs(title = "Regressing X onto Y") +
  theme_minimal()
```
Let's check that our linear regression assumptions hold. One way to do this is by plotting the residuals.

```{r}
ggplot(data = data.frame(
  fitted = fitted(ols1), # predicted values of Y for each value of X
  resid  = resid(ols1)
), aes(x = fitted, y = resid)) +
  geom_point(color = "grey20", alpha = 0.6) +        # scatterplot
  geom_hline(yintercept = 0, color = "purple", size = 1.2) + # reference line
  labs(
    title = "Plotting OLS Residuals",
    x = "Fitted",
    y = "Residuals"
  ) +
  theme_minimal()
```

The fitted values are the values of Y our regression line predicts for each value of X in our dataset. The residuals show how far each predicted value of Y is from the actual value of Y for that observation. 

Use this plot to check that:

* At every fitted value of Y, the residuals average to about 0. 

* The spread of the residuals is about the same at every fitted value of Y.

There are also many statistical tests (and R packages to run them!) that will formally test whether your regression assumptions are valid. These can be found with a quick internet search depending on what your specific testing needs are (for example, some of them are summarized [here](https://book.stat420.org/model-diagnostics.html)).

## Exercise: Practice Running OLS in R (10 minutes)

For this exercise we will use a new simulated dataset:

```{r}
# set seed
set.seed(3636)

# creating a toy dataset
x = runif(n = 500) * 5
y = y = 3 + 5 * x + rnorm(n = 500, mean = 0, sd = x)
df2 <- data.frame(x, y)
```

1. Create a scatter plot to look at the relationship between X and Y.

```{r}
df2 %>% 
ggplot(., aes(x = x, y = y)) +
  geom_point(color = "grey20", alpha = 0.6) + 
  labs(title = "X & Y Linearly Related") +
  theme_minimal()
```

2. Run a linear regression.

```{r}
# running regression
ols2 <- df2 %>% 
  lm(y ~ x, data = .)

# checking results
ols2 %>% summary()
```

3. Check if any of the linear regression assumptions are violated.

```{r}
ggplot(data = data.frame(
  fitted = fitted(ols2), # predicted values of Y for each value of X
  resid  = resid(ols2)
), aes(x = fitted, y = resid)) +
  geom_point(color = "grey20", alpha = 0.6) +        # scatterplot
  geom_hline(yintercept = 0, color = "purple", size = 1.2) + # reference line
  labs(
    title = "Plotting OLS Residuals",
    x = "Fitted",
    y = "Residuals"
  ) +
  theme_minimal()
```

Homoskedasticity (constant variance of errors accross observed values) is violated. 

4. Interpret the results of your regression (non-causally).

For every one unit increase in X, we observe on average a 5.086 unit increase in Y. 

# Data Simulation and How to Simulate Regressions

Data simulation is the opposite of data analysis. You create a data set through simulation and then "understand" it using the analytically tools you learned. 

## Advantages for doing data simulation  
  
  1. *The truth is known*: We know the values of simulated parameters, and we can therefore compare them with the model estimates. This gives us a better understanding of how well the model performs.  
  
  2. *Understanding parameters through tuning*: Sometimes the effect of one or more parameters is unclear. Being able to tune the parameters and observe how they affect the resultant data helps us better understand the parameters.  
  
  3. *Evaluating the bias and efficiency of statistics*: As we have practiced in PS1, by simulating a virtual population, we can directly observe how the sampling procedure affects the variability of the sample statistics. In general, through simulation, we can generate sampling distribution of any statistics we are interested in, and evaluate their bias and efficiency.  
  
  4. *Understanding models*: Finally, data simulation provides proof that you understand a model: If you can simulate data under a certain model, then it is likely that you really understand that model. 

## Simulation from a stochastic process

For a social system, we always take into account the randomness in social life when we simulate data. In other words, the data is generated from a **stochastic process** in which the state of the system cannot be precisely predicted given its current state and even with a full knowledge of all the factors affecting that process.  

In concrete, it means that we always include an error/residual term when simulating a social process. This error/residual term conveys the unpredictability and randomness of social lives. 

## Simulate a bivariate relationship
  
For example, let's simulate a **population** data in which the *years of education* affect one's *income rank* according to the following equation: 
$$I_i = 10 + 6 \cdot E_i + \epsilon_i$$  
  
We will first simulate *years of education* - the independent variable (IV), then *income rank* - the dependent variable (DV) according to the above equation.  
  
We simulate years of education using `rpois()`, a function that generates a random Poisson distribution with a specified parameter $\lambda$. You can learn more about the distribution [here](https://en.wikipedia.org/wiki/Poisson_distribution).  
  
```{r part1-simulateIV, fig.align = "center"}

## simulate IV (edu level)
set.seed(1234)
edu <- rpois(30000, lambda = 6) ## rpois: Random Poisson Distribution with parameter lamda 

## lot histogram of IV
edu %>% 
  as_tibble() %>%
  ggplot(aes(value)) +
  geom_histogram(color = "black", fill = "grey", binwidth = 1) +
  labs(x = "Years of Education") + 
  theme_bw()

## summary statistics
summary(edu)
```
  
  When simulating earning, since we are simulating a stochastic process, we always add an error term, noted as $\epsilon_i$ in the above equation. Note that normally this error term is modeled using `rnorm()` assuming **the error term is normally distributed with a mean of 0 and a sd that is a constant value**, so that the errors vary following the same random pattern across all values of the IV. 
  
  But we might need to change this when we want to simulate data that **violate the homoskedasticity assumption**, which means the error term is not purely random but dependent on the value of the IV. 

```{r part1-simulateDV, fig.align = "center"}

## simulate DV 
set.seed(1234)
earn <- 10 + 6*edu + rnorm(30000, 0, 10) ## add a random error using rnorm() 

## plot histogram of DV
earn %>% 
  as_tibble() %>%
  ggplot(aes(value)) +
  geom_histogram(color = "black", fill = "grey", binwidth = 5) +
  labs(x = "Income Rank") +
  theme_bw()

## summary statistics
summary(earn)

## combine data frame
df <- data.frame(x_edu = edu, 
             y_earn = earn)

## summary statistics of the data frame
describe(df)

## scatter plot with a fitted lm line
df %>%
  ggplot(aes(x = x_edu, y = y_earn)) +
  geom_point(shape = 1, alpha = 0.7) +
  geom_smooth(method = "lm") +
  labs(title = "Relationship Between Years of Education and Income Rank",
       subtitle = "(using simulated data)",
       x = "Years of Education",
       y = "Income Rank") +
  theme_bw()

```

## Fit OLS to sampled data
  
 We can fit a regression model to the sampled data from the simulated population, and compare the result with the "true" relationship. As you can see our modeling result is quite close to the "true" parameter values. 

```{r part1-fit-ols}

## sample 300 obs
sample <- df[sample(1:dim(df)[1], 300), ]

## run a model
m_simu <- lm(y_earn ~ x_edu, data = sample)

summary(m_simu)
```
  
## `stargazer` for regression tables  
  
In displaying regression tables, a commonly used package is `stargazer`, its main function `stargazer()` can format modeling results to neat regression tables. You can set the output type `type = "latex"` and copy the output to Overleaf or other LaTeX editors, the code will automatically render to a cleanly formatted regression table. You should also set the type to `type = "latex"` when knitting to a PDF document. For knitting to HTML, set `type = "html"`.
  
```{r stargazer}
## use stargazer to display regression tables
## for quick view in R:
stargazer(m_simu, type = "text")

## for PDF: 
## stargazer(m_simu, type = "latex")

```

A nice feature of `stargazer` is that it allows you to customize what statistics to add or omit in your table. For example, if we want to omit Residual Std. Error and the F Statistic:

```{r stargazer2,eval=TRUE}

## omit some statistics
stargazer(m_simu, m_simu, type="text", omit.stat=c("ser", "f"),
          dep.var.labels = "Earning",
          covariate.labels = c("Education","Constant"))

```

Take a look at p.22 [on stargazer's documentation](https://cran.r-project.org/web/packages/stargazer/stargazer.pdf). You will find all statistics abbreviations you can use for omission.

You can also change the dependent variable label, independent variable labels, column labels, etc. Try to Google them when you want to customize.

## Exercise: Simulating and Running Regression (20 minutes)

1. Simulate the population data with the same relationship as specified above, only **changing the residual term from having a sd = 10 to sd = 50**. Follow the steps demonstrated above, create a sampled dataframe and fit an OLS model to your sampled data. Create a scatterplot of your data. *Note:* Remember to `set.seed()`! 

```{r part1-exercise-Q1}
## your simulation

## simulate IV (education level) 
set.seed(3636)
edu <- rpois(30000, lambda = 6) ## rpois: Random Poisson Distribution with parameter lamda 

## simulate DV (income level) 
earn <- 10 + 6*edu + rnorm(30000, 0, 50) ## add a random error using rnorm() 

# combining to form dataframe
df <- data.frame(x_edu = edu, 
             y_earn = earn)

## sample 300 obs
sample <- df[sample(1:dim(df)[1], 300), ]

## run a model
m_simu2 <- lm(y_earn ~ x_edu, data = sample)
```

```{r}
# scatter plot
df %>%
  ggplot(aes(x = x_edu, y = y_earn)) +
  geom_point(shape = 1, alpha = 0.7) +
  geom_smooth(method = "lm") +
  labs(title = "Relationship Between Years of Education and Income Rank",
       subtitle = "(using simulated data)",
       x = "Years of Education",
       y = "Income Rank") +
  theme_bw()
```


2. Then, using the formula we have learned in the lecture, calculate the following statistics in `R`:  
  (a) The value of TSS
  (b) $R^2$
  (c) $se_{\beta_1}$
  (d) Construct the 95% confidence interval for $\beta_1$ (You can use the $\beta_1$ value reported in your OLS modeling result)

```{r part1-exercise-Q2}

## TSS
tss <- sum((sample$y_earn - mean(sample$y_earn))^2)
tss

## SSE
sse <- sum((sample$y_earn - m_simu2$fitted.values)^2)
sse

## R^2

(tss - sse) / tss

## SE of beta1

mse <- sse / (nrow(sample) - 2)

se_beta1 <- sqrt(mse / sum((sample$x_edu - mean(sample$x_edu))^2))

se_beta1

## 95% CI

ci_low <- m_simu2$coefficients[2] - (1.96*se_beta1)
ci_high <- m_simu2$coefficients[2] + (1.96*se_beta1)

ci_low
ci_high
```

- The **sum of square error (SSE)** for Y is the sum of square errors for the fitted OLS model: 
  $$SSE = \sum\epsilon^2 = \sum(y - \hat{y})^2$$

- The **total sum of squares (TSS)** for Y is the sum of square errors for the baseline OLS model (the "null model") that predict the value of Y without any X (the mean of Y, $\bar{Y}$, is used in the null model): 
  $$TSS = \sum(y - \bar{y})^2$$  
  
- **R-squared** (coefficient of determination) is the proportional reduction in squared error when fitted with the OLS model in comparison with the null model: 
  $$R^2 = \frac{TSS - SSE}{TSS}$$ 
  
- The **mean square error (MSE)**: 
  $$MSE = \frac{SSE}{n-2}$$  
  
- The **standard error of $\beta_1$ in bivariant OLS** regression: 
  $$se_{\beta_1} = \sqrt{\frac{MSE}{\sum{(x -\bar{x})^2}}}$$

3. Compare the scatterplot, the estimated $\beta_0$ and $\beta_1$, the $R^2$, and the $se_{\beta_1}$ in the demo OLS model output and your own simulated data model output, what do you observe? Why?  

```{r part1-exercise-Q3} 

## our code here

summary(m_simu)
summary(m_simu2)
```

The $\beta_0$ and the $\beta_1$ are about the same, because the numbers were drawn from populations that had a relatively similar relationship between income rank and education (i.e. we created income rank to be roughly education*6 in both simulations). They differ slightly because of random chance with the sample we drew and a different data generating process for error terms. The R^2 is much larger and the $se_{\beta_1}$ is smaller for the first model because we created it with errors that are less variable. 

# Part 2: Multivariate Regression Using R

## Simulate Population

Suppose our outcome variable, $Y_i$, has the following data generating process in relation to education and income: 
  $$Y_i = 10 + 5 \cdot E_i + 3 \cdot I_i - 2\cdot E_i \cdot I_i + \epsilon_i$$

Education is a random variable varies between 6 and 14 following a uniform distribution.  

Income is a random variable varies between 3 and 10 following a uniform distribution. 
  
The error term, $\epsilon$ follows a normal distribution with mean = 0 and sd = 2. 
  
```{r part2-simulateY, fig.align = "center"}

## simulate the data generating process for the population
set.seed(1234)
n <- 30000
edu <- runif(n, min = 6, max = 14) %>% ceiling() ## ceiling the value to get only integers
inc <- runif(n, min = 3, max = 10)
y <- 10 + 5*edu + 3*inc + (-2)*edu*inc + rnorm(n, mean = 0, sd = 2)

## combine variables into a data frame
df <- data.frame(edu = edu, inc = inc, y = y)

## check distribution
par(mfrow = c(1, 3)) ## arrange the following plots in 1 row and 3 cols
hist(edu)
hist(inc)
hist(y)

## sample
set.seed(1234)
sample <- df[sample(1:dim(df)[1], 300), ]
```

## Multiple regression for a sample with no interaction term

First, let us estimate a multiple regression without interaction for a sample, and see how the OLS line fit in relation to the data. We also want to check how the model residuals behave in relation to fitted y. 

$$
y_i = 10 + 3 \cdot I_i + 5 \cdot E_i +  e_i
$$

```{r part2-multi-regression}

## model without interaction
mod1 <- lm(y ~ edu + inc, data = sample)
summary(mod1)

## a scatterplot of edu and y, with a fitted OLS line with income taking its mean value
sample %>%
  ggplot() +
  geom_point(aes(edu, y, color = inc)) +
  geom_abline(intercept = mod1$coefficients[1] + mod1$coefficients['inc']*mean(inc), 
              slope = mod1$coefficients['edu']) +
  theme_bw() +
  labs(title = "Fitted OLS line with income held at its mean value")

## a scatterplot of inc and y, with a fitted OLS line with edu taking its mean value
sample %>%
  ggplot() +
  geom_point(aes(inc, y, color = edu)) +
  geom_abline(intercept = mod1$coefficients[1] + mod1$coefficients['edu']*mean(df$edu) , 
              slope = mod1$coefficients['inc']) +
  theme_bw()

## check model residual behaviors
## residual vs. fitted y
plot(mod1$fitted.values, mod1$residuals)
abline(h = 0, col="red")

```

As we can see, when we did not taking into consideration of the interaction term, the model still captures the average effect of the predictor (while holding the other at constant). However, the scatterplot as well as the residual diagnostic also suggest that we are missing some important patterns. 

## Multiple regression with interaction:

Now we add the interaction term `edu*inc` into the model. We can plot the relationship between y and one of the x's and also plot the different slopes given the other x changes. 

```{r part2-multi-regression2}

## taking into account of the interaction:
mod2 <- lm(y ~ edu + inc + edu:inc, data = sample)
summary(mod2)

## relationship between inc and y when edu is changing:
sample %>%
  mutate(edu = as.factor(edu)) %>%
  ggplot(aes(inc, y, color = edu)) +
  geom_point() +
  geom_abline(intercept = mod2$coefficients[1] + mod2$coefficients['edu']*edu,
              slope = mod2$coefficients['inc'] + mod2$coefficients['edu:inc']*edu) +
  theme_bw() +
  labs(title = "Income and Y by education levels with fitted OLS line (with interaction)")

## check model residual behaviors
## residual vs. fitted y
plot(mod2$fitted.values, mod2$residuals)
abline(h = 0, col="red")
```

## Interpret regression coefficient for model with interactions:

Interaction effects can be tricky to interpret. The safest way is to write down your models in equations and plug in values into the equations to figure out the difference. 

For example, for Model 2, our regression equation is:

$$\hat{y}_i = 9.03 + 4.78 \cdot E_i + 2.51\cdot I_i - 2.00 \cdot E_i \cdot I_i$$

Note that for the effect of `edu` and `inc`, it is no longer indicated by the coefficients of their individual terms. 
**Whenever you try to interpret the coefficient for variables that are included in an interaction term, you need to take into account the coefficient of the interaction term**. 

For example, let's examine the effect of `edu`. Holding all other variables constant, for **one unit increase in `edu`**, we have:

$$\Delta\hat{y} = 5.03 - 2.00\cdot I$$
This means that **the effect of education depends on income**. And the coefficient in the term $5.03 \cdot E_i$ can only be interpreted as the effect of income **when income equals to 0**.  

Let us look at the graphs on the effects of education on $y_i$ when there is an interaction effect between education and income:

```{r part2-plot}

## relationship between inc and y when edu is changing:
sample %>%
  ggplot(aes(inc, y, color = as.factor(edu))) +
  geom_point() +
  geom_abline(intercept = mod2$coefficients[1] + mod2$coefficients['edu']*edu,
              slope = mod2$coefficients['inc'] + mod2$coefficients['edu:inc']*edu,
              color = as.factor(edu)) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  xlim(-5, 25) +
  ylim(-200, 200) +
  theme_bw() +
  labs(title = "Effect of Education when Income = 0")

```

## Exercise (bonus if time permits)

+ Import `lab5_earnings.csv` to your environment. Perform the following data cleaning steps: (1) If `age` takes the value 9999, recode it as `NA`; (2) Create a new variable `female` that equals 1 when `sex` takes the value `female`, and equals to 0 otherwise; (3) Create a new variable `black` that equals 1 when `race` is `black` and equals to 0 otherwise; (4) Create a new variable `other` that equals to 1 when `race` is 'other` and 0 otherwise.
  
+ Use the `describe()` function from the `psych` package to generate a quick descriptive statistics of your data.
  
+ Now, estimate the following models and display your model results in a single table using `stargazer(m_1, m_2, ..., m_n, type="text")`.

(1) Model 1: earn ~ age (baseline)
(2) Model 2: earn ~ age + edu 
(3) Model 3: earn ~ age + edu + female
(4) Model 4: earn ~ age + edu + female + race
(5) Model 5: earn ~ age + edu + female + race + edu*female

+ In Model 5, holding other variables constant, what will be the predicted difference in estimated mean earnings for a white man and a white women?  
  
+ In Model 5, holding other variables constant, what will be the predicted difference in estimated mean earnings for a white women and a black women?  
  
+ In Model 5, Holding other variables constant, what will be the predicted difference in estimated mean earnings for a white man and a black women?

```{r p2}
## our code here
earnings <- read.csv("data/lab5_earnings.csv")

## recode
earnings[earnings$age == 9999, "age"] <- NA
earnings <- earnings %>% 
  mutate(age = 
           case_when(age == 9999 ~ NA,
                     .default = age))

## create dummies for female
earnings <-
  earnings %>%
  mutate(female =
           case_when(sex == "female" ~ 1,
                     .default = 0))

## create dummies for black
earnings <-
  earnings %>%
  mutate(black = case_when(race == "black" ~ 1,
                           .default = 0),
         other = case_when(race == "other" ~ 1,
                           .default = 0))

## fit model 5
lm5 <- lm(earn ~ age + edu + female + black + other + edu*female,
   data = earnings)

summary(lm5)

```

